{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad8fba43-6518-4d13-a0da-4559269a4133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6218545b-a885-413c-a859-7141a7e589a7",
   "metadata": {},
   "source": [
    "# load baseline model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae42679-845c-4c98-a831-aa737f7ffa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'base/checkpoint-6111'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e460437-9e5e-4b41-bd4b-0823bcc985af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2d1a6a-dd1d-4d26-9758-73d3f175a318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bf1d3e-803f-4ceb-98af-be295d5412d7",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b426444b-5dd4-48e9-9397-c1738745baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load Data\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "file_path = '../MISC/Dataset/squad_v2_with_explanations.json'\n",
    "data = load_data(file_path)\n",
    "train_data = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e65fe28-511d-4157-9c2d-2fe9a223dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data to be converted into the format required by the datasets library\n",
    "def extract_qas(data):\n",
    "    questions = []\n",
    "    contexts = []\n",
    "    answers = []\n",
    "    explanations = []  \n",
    "    is_impossible = []\n",
    "\n",
    "    for article in data:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                # Check if an explanation exists, skip the current question if not\n",
    "                explanation = qa.get('explanation')\n",
    "                if explanation is None:\n",
    "                    continue  \n",
    "\n",
    "                questions.append(qa['question'])\n",
    "                contexts.append(context)  \n",
    "                explanations.append(explanation)  \n",
    "                is_impossible.append(qa['is_impossible'])\n",
    "\n",
    "                # answer\n",
    "                if qa['is_impossible']:\n",
    "                    \n",
    "                    if 'plausible_answers' in qa:\n",
    "                        answers.append(qa['plausible_answers'][0]['text'])\n",
    "                    else:\n",
    "                        answers.append(\"\")  \n",
    "                else:\n",
    "                    if qa['answers']:\n",
    "                        answers.append(qa['answers'][0]['text'])\n",
    "                    else:\n",
    "                        answers.append(\"\")  \n",
    "\n",
    "    return {\n",
    "        'question': questions,\n",
    "        'context': contexts,\n",
    "        'answer': answers,\n",
    "        'explanation': explanations,\n",
    "        'is_impossible': is_impossible\n",
    "    }\n",
    "\n",
    "# processe data\n",
    "processed_data = extract_qas(train_data)\n",
    "dataset = Dataset.from_dict(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5239906f-f2f0-4e56-b58f-e49242fb6554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'answer', 'explanation', 'is_impossible'],\n",
       "    num_rows: 13401\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f45a72f4-2b3c-4cf8-b4c3-d2b0b07d0b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What category of game is Legend of Zelda: Australia Twilight?',\n",
       " 'context': 'The Legend of Zelda: Twilight Princess (Japanese: ゼルダの伝説 トワイライトプリンセス, Hepburn: Zeruda no Densetsu: Towairaito Purinsesu?) is an action-adventure game developed and published by Nintendo for the GameCube and Wii home video game consoles. It is the thirteenth installment in the The Legend of Zelda series. Originally planned for release on the GameCube in November 2005, Twilight Princess was delayed by Nintendo to allow its developers to refine the game, add more content, and port it to the Wii. The Wii version was released alongside the console in North America in November 2006, and in Japan, Europe, and Australia the following month. The GameCube version was released worldwide in December 2006.[b]',\n",
       " 'answer': 'action-adventure',\n",
       " 'explanation': \"The category of game that The Legend of Zelda: Twilight Princess falls into is 'action-adventure.'\",\n",
       " 'is_impossible': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2075]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6556804-ccec-4e10-aad9-01174032a0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_t5_explanations(examples):\n",
    "    questions = examples['question']\n",
    "    contexts = examples['context']\n",
    "    explanations = examples['explanation']\n",
    "    answers = examples['answer']\n",
    "\n",
    "    # Predict and explain first\n",
    "    source_texts = [f\"question: {q} context: {c}\" for q, c, a in zip(questions, contexts, answers)]\n",
    "    target_texts = [f\"explanations: {e}\" for e, a in zip(explanations, answers)]\n",
    "    # target_texts = [exp if exp else \"no explanation\" for exp in explanations]\n",
    "\n",
    "    # Use tokenizer to process batch data\n",
    "    model_inputs = tokenizer(source_texts, max_length=768, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    model_outputs = tokenizer(target_texts, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": model_inputs['input_ids'],\n",
    "        \"attention_mask\": model_inputs['attention_mask'],\n",
    "        \"labels\": model_outputs['input_ids']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01624715-3945-4dee-82f3-6657ecd91d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a493404cfa464ada859012e6f4f2a4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_dataset = dataset.map(preprocess_t5_explanations, batched=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ce06cf3-ffa9-4e4b-8e57-b4d5757d54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb8473e4-cd06-443e-95ff-5ac8fb7c77d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq, T5Tokenizer\n",
    "\n",
    "# Define training parameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./t5-two-stage-1',\n",
    "    # evaluation_strategy='epoch',  # Evaluation strategy set to 'epoch'\n",
    "    save_strategy = 'epoch',\n",
    "    # evaluation_strategy='steps',  # Evaluation strategy set to 'epoch'\n",
    "    # eval_steps=20,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=False,     \n",
    "    logging_steps=1000,           \n",
    ")\n",
    "\n",
    "# data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "#Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed1cc05f-ec11-4098-ae8f-d69c90655b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 07:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=630, training_loss=0.6480337960379464, metrics={'train_runtime': 457.4106, 'train_samples_per_second': 87.893, 'train_steps_per_second': 1.377, 'total_flos': 8161719666868224.0, 'train_loss': 0.6480337960379464, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a44c91-8aa6-4f56-b1b0-8a2dad5f0b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f354d5a-14cd-4420-aca3-1a145733094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_t5_explanations(examples):\n",
    "    questions = examples['question']\n",
    "    contexts = examples['context']\n",
    "    explanations = examples['explanation']\n",
    "    answers = examples['answer']\n",
    "\n",
    "    # Add explanation prediction answer\n",
    "    source_texts = [f\"question: {q} context: {c} explanations: {e}\" for q, c, e in zip(questions, contexts, explanations)]\n",
    "    target_texts = [f\"{a}\" for e, a in zip(explanations, answers)]\n",
    "    # target_texts = [exp if exp else \"no explanation\" for exp in explanations]\n",
    "\n",
    "    # Use tokenizer to process batch data\n",
    "    model_inputs = tokenizer(source_texts, max_length=768, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    model_outputs = tokenizer(target_texts, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": model_inputs['input_ids'],\n",
    "        \"attention_mask\": model_inputs['attention_mask'],\n",
    "        \"labels\": model_outputs['input_ids']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a71e7c9f-e95b-4903-add6-14c626b1a4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3850242de7744248810a68d14e122015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13401 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_dataset = dataset.map(preprocess_t5_explanations, batched=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f84be45-9bd6-4349-a602-f53cfae188f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq, T5Tokenizer\n",
    "\n",
    "# Define training parameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./t5-two-stage-2',\n",
    "    # evaluation_strategy='epoch',  # Evaluation strategy set to 'epoch'\n",
    "    save_strategy = 'epoch',\n",
    "    # evaluation_strategy='steps',  # Evaluation strategy set to 'epoch'\n",
    "    # eval_steps=20,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=False,     \n",
    "    logging_steps=1000,           \n",
    ")\n",
    "\n",
    "# data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "#Initialize the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "954470f2-0289-4a4a-8fdc-ea573f9492f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 07:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=630, training_loss=0.015314376164996434, metrics={'train_runtime': 457.5371, 'train_samples_per_second': 87.868, 'train_steps_per_second': 1.377, 'total_flos': 8161719666868224.0, 'train_loss': 0.015314376164996434, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930294d-c2dd-4a0a-bee2-70d982369f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36950f1e-f7e8-4716-a119-24686f21a180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: 10th\n",
      "truth: 10th\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define problem and context\n",
    "context = \"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\\\"Norman\\\" comes from \\\"Norseman\\\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\"\n",
    "\n",
    "question = \"What century did the Normans first gain their separate identity?\"\n",
    "truth = \"10th\"\n",
    "# Encode the input text using a tokenizer and make sure the input data is also on the correct device\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "# Generate answer\n",
    "outputs = model.generate(input_ids, max_length=40)\n",
    "\n",
    "# Convert the output token IDs to text\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Answer:\", answer)\n",
    "print(\"truth:\", truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "515d68fe-a54f-4207-9ee1-26509efd9d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: Rollo\n",
      "truth: Rollo\n"
     ]
    }
   ],
   "source": [
    "question = \"Who was the Norse leader?\"\n",
    "truth = \"Rollo\"\n",
    "\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=40)\n",
    "\n",
    "\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Answer:\", answer)\n",
    "print(\"truth:\", truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dccb1a-1819-4a1e-8fac-e48854fa85f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
