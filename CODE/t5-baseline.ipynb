{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e7626b4-86c5-4244-8297-fbcdd8dac5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e2789b5-4bf9-4fec-b4ca-407224f7525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import os\n",
    "\n",
    "# result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "# output = result.stdout\n",
    "# for line in output.splitlines():\n",
    "#     if '=' in line:\n",
    "#         var, value = line.split('=', 1)\n",
    "#         os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d84a03-7d87-4687-8cd7-1ae01d8db59b",
   "metadata": {},
   "source": [
    "# load base modal and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d90dd8eb-58cb-4a80-83b0-1a6179b3bab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93ab53dc-1aeb-420a-b249-7d1126b05fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca41d644-22fa-4f6a-87b6-ebf9c10885fd",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e999783-dc65-42c5-a5f7-71bb3a510dee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "file_path = '../MISC/Dataset/train-v2.0.json'\n",
    "data = load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0657def-a041-4cfb-b9e6-9c3932c10c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cbfd5e9-d4b6-46bd-aef4-61a9d8d8d641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare data to be converted into the format required by the datasets library\n",
    "def extract_qas(data):\n",
    "    questions = []\n",
    "    contexts = []\n",
    "    answers = []\n",
    "    is_impossible = []\n",
    "\n",
    "    for article in data:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                questions.append(qa['question'])\n",
    "                contexts.append(context)  # Reuse the same context\n",
    "                is_impossible.append(qa['is_impossible'])\n",
    "\n",
    "                # Answers\n",
    "                if qa['is_impossible']:\n",
    "                    if 'plausible_answers' in qa and qa['plausible_answers']:\n",
    "                        answers.append(qa['plausible_answers'][0]['text'])\n",
    "                    else:\n",
    "                        answers.append(\"\")  \n",
    "                else:\n",
    "                    if qa['answers']:\n",
    "                        answers.append(qa['answers'][0]['text'])\n",
    "                    else:\n",
    "                        answers.append(\"\")  \n",
    "\n",
    "    return {\n",
    "        'question': questions,\n",
    "        'context': contexts,\n",
    "        'answer': answers,\n",
    "        'is_impossible': is_impossible\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  processe data\n",
    "processed_data = extract_qas(train_data)\n",
    "dataset = Dataset.from_dict(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a81bc795-fa69-48fd-a85d-54208e6c1149",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'answer', 'is_impossible'],\n",
       "    num_rows: 130319\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff2af591-3918-439b-8c56-dcbf7acbdd45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa93b2c688d4100978160c01c618706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_t5_qa(examples):\n",
    "    # Combine questions and context into the format required for model input\n",
    "    questions = examples['question']\n",
    "    contexts = examples['context']\n",
    "    source_texts = [f\"question: {q} context: {c}\" for q, c in zip(questions, contexts)]\n",
    "\n",
    "    answers = examples['answer']\n",
    "    target_texts = [ans if ans else \"no answer\" for ans in answers]\n",
    "\n",
    "    # Use tokenizer to process batch data\n",
    "    model_inputs = tokenizer(source_texts, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    model_outputs = tokenizer(target_texts, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    # print(\"Input IDs shape:\", model_inputs['input_ids'].shape)\n",
    "    # print(\"Labels shape:\", model_outputs['input_ids'].shape)\n",
    "    return {\n",
    "        \"input_ids\": model_inputs['input_ids'],\n",
    "        \"attention_mask\": model_inputs['attention_mask'],\n",
    "        \"labels\": model_outputs['input_ids']\n",
    "    }\n",
    "\n",
    "\n",
    "processed_dataset = dataset.map(preprocess_t5_qa, batched=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c38f8b85-3db9-489e-b76b-07a5800b8501",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'answer', 'is_impossible', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 130319\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0713de4a-81bd-4c92-9dc4-5c9d955b27f8",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46210187-b658-438c-9bb4-939ca4c1fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Convert NumPy arrays to Python lists and then to strings\n",
    "    predictions = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions.tolist()]\n",
    "    labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels.tolist()]\n",
    "\n",
    "    # Ensure predictions and labels are text data\n",
    "    predictions = [pred.strip() for pred in predictions]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # Compute EM and F1 scores\n",
    "    squad_metric = load_metric(\"squad\")\n",
    "    results = squad_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    return {\"exact_match\": results[\"exact_match\"], \"f1\": results[\"f1\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3364e-c3c9-4c71-935c-80e7079a3c1a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13e0af16-a99e-422b-be09-89bb146e1e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./base',\n",
    "    # evaluation_strategy='epoch',  # Evaluation strategy set to 'epoch'\n",
    "    save_strategy = 'epoch',\n",
    "    # evaluation_strategy='steps',  # Evaluation strategy set to 'epoch'\n",
    "    # eval_steps=20,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=3,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model='f1',\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13509fd2-7c76-4db5-a8e8-bdc764883dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6111' max='6111' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6111/6111 46:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.757500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.085700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.020300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.020300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.019900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.019400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.019400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.019200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.018500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6111, training_loss=0.08271375485526582, metrics={'train_runtime': 2814.5731, 'train_samples_per_second': 138.905, 'train_steps_per_second': 2.171, 'total_flos': 5.29128246780887e+16, 'train_loss': 0.08271375485526582, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878a1b9-f867-40ee-81ef-7329cd51299c",
   "metadata": {},
   "source": [
    "# Command line test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab62ef65-5319-46ef-9b56-27b11ac94a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: 10th\n",
      "truth: 10th\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define problem and context\n",
    "context = \"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\\\"Norman\\\" comes from \\\"Norseman\\\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\"\n",
    "\n",
    "question = \"What century did the Normans first gain their separate identity?\"\n",
    "truth = \"10th\"\n",
    "# Encode the input text using a tokenizer and make sure the input data is also on the correct device\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "# Generate answer\n",
    "outputs = model.generate(input_ids, max_length=40)\n",
    "\n",
    "# Convert the output token IDs to text\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Answer:\", answer)\n",
    "print(\"truth:\", truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bd51c81-1348-42fe-8925-2934f3346b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: King Charles III of West Francia\n",
      "truth: Rollo\n"
     ]
    }
   ],
   "source": [
    "question = \"Who was the Norse leader?\"\n",
    "truth = \"Rollo\"\n",
    "\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=40)\n",
    "\n",
    "\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Answer:\", answer)\n",
    "print(\"truth:\", truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a283adfe-6e03-4b9c-96ea-c2eb26fe5b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: Denmark, Iceland and Norway\n",
      "truth: Denmark, Iceland and Norway\n"
     ]
    }
   ],
   "source": [
    "question = \"From which countries did the Norse originate?\"\n",
    "truth = \"Denmark, Iceland and Norway\"\n",
    "\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=40)\n",
    "\n",
    "\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Answer:\", answer)\n",
    "print(\"truth:\", truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe2987dd-25a4-4f7a-ba81-2e9ee6a86e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: France\n",
      "truth: France\n"
     ]
    }
   ],
   "source": [
    "question = \"In what country is Normandy located?\"\n",
    "truth = \"France\"\n",
    "\n",
    "input_text = f\"question: {question} context: {context}\"\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=40)\n",
    "\n",
    "\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Answer:\", answer)\n",
    "print(\"truth:\", truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
