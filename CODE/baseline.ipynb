{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVpTw_P5MEOb"
      },
      "source": [
        "# Rule-base Baseline\n",
        "\n",
        "First set up the import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JgNaqzuYMO1C"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from functools import lru_cache\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5lHte_sMPy3"
      },
      "source": [
        "Download necessary NLTK data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VjcKLr-MVrw",
        "outputId": "b56c6b87-15ab-4abc-bf56-3099025dddc8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\steve\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\steve\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\steve\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ensure necessary NLTK data is downloaded\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znlLK7IbMdnQ"
      },
      "source": [
        "Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9hpumN5KMkqf"
      },
      "outputs": [],
      "source": [
        "def load_data(filepath):\n",
        "    \"\"\"Load JSON data from a file.\"\"\"\n",
        "    with open(filepath, 'r') as file:\n",
        "        return json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkkanW9iMelY"
      },
      "source": [
        "WordNet Semantic Similarity function setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3ZNUxstOMXYh"
      },
      "outputs": [],
      "source": [
        "@lru_cache(maxsize=10000)\n",
        "def get_synsets(word):\n",
        "    \"\"\"Fetch and cache synsets for a word to reduce WordNet lookup overhead.\"\"\"\n",
        "    return wn.synsets(word)\n",
        "\n",
        "@lru_cache(maxsize=50000)\n",
        "def word_similarity(word1, word2):\n",
        "    \"\"\"Calculate maximum similarity score between synsets of two words with caching.\"\"\"\n",
        "    synsets1 = get_synsets(word1)\n",
        "    synsets2 = get_synsets(word2)\n",
        "    if not synsets1 or not synsets2:\n",
        "        return 0\n",
        "    max_sim = max((wn.path_similarity(syn1, syn2) or 0) for syn1 in synsets1 for syn2 in synsets2)\n",
        "    return max_sim\n",
        "\n",
        "def sentence_similarity(sentence1, sentence2):\n",
        "    \"\"\"Calculate semantic similarity between two sentences using WordNet.\"\"\"\n",
        "    words1 = word_tokenize(sentence1.lower())\n",
        "    words2 = word_tokenize(sentence2.lower())\n",
        "    total_score = 0\n",
        "    count = 0\n",
        "\n",
        "    for word1 in words1:\n",
        "        for word2 in words2:\n",
        "            sim_score = word_similarity(word1, word2)\n",
        "            if sim_score > 0:\n",
        "                total_score += sim_score\n",
        "                count += 1\n",
        "\n",
        "    return total_score / count if count else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H24O1UMcMnCL"
      },
      "source": [
        "Evaluate Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QAdSLdoRzYh"
      },
      "source": [
        "\n",
        "\n",
        "**1.Baseline 1: Direct Word Overlap**\n",
        "\n",
        "This method is try to determine if the question is answerable based on the direct presence of its words in the context.\n",
        "\n",
        "This baseline is straightforward and assumes that if a question's vocabulary substantially overlaps with the context, and the context likely contains the information needed to answer the question.\n",
        "\n",
        "**2. Sentence-Level Word Overlap**\n",
        "\n",
        "This method considers the distribution of question words across individual sentences in the context.\n",
        "\n",
        "**3. WordNet Semantic Similarity**\n",
        "\n",
        "This method check the answerability of a question bases on the semantic similarity between the words in the question and those in the context, using the relationships defined in WordNet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PWBxmz5gMqvi"
      },
      "outputs": [],
      "source": [
        "def evaluate_baseline(test_data, output_csv):\n",
        "    \"\"\"Evaluate the WordNet similarity baseline and write results to CSV including comparison and correctness percentages.\"\"\"\n",
        "    correct_counts = {'Baseline 1': 0, 'Baseline 2': 0, 'Baseline 3': 0}\n",
        "    total_questions = 0\n",
        "\n",
        "    with open(output_csv, mode='w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['Id', 'Baseline 1 Category', 'Baseline 2 Category', 'Baseline 3 Category', 'Is Impossible'])\n",
        "\n",
        "        # Process each question with a progress bar\n",
        "        questions = [(qa, article['paragraphs']) for article in test_data['data'] for paragraph in article['paragraphs'] for qa in paragraph['qas']]\n",
        "        for qa, paragraphs in tqdm(questions, desc=\"Evaluating questions\"):\n",
        "            context = paragraphs[0]['context'].lower()  # Assuming one context per paragraph\n",
        "            question_id = qa['id']\n",
        "            question = qa['question'].lower()\n",
        "            is_impossible = qa['is_impossible']\n",
        "\n",
        "            # Baseline 1: Direct Word Overlap\n",
        "            question_words = word_tokenize(question)\n",
        "            context_words = word_tokenize(context)\n",
        "            included_words = sum(1 for word in question_words if word in context_words)\n",
        "            total_words = len(question_words)\n",
        "            category_b1 = 1 if total_words > 0 and included_words / total_words >= 0.5 else 0\n",
        "\n",
        "            # Baseline 2: Sentence-Level Word Overlap\n",
        "            sentences = nltk.sent_tokenize(context)\n",
        "            max_overlap = 0\n",
        "            for sentence in sentences:\n",
        "                sentence_words = word_tokenize(sentence)\n",
        "                included_words = sum(1 for word in question_words if word in sentence_words)\n",
        "                if included_words / total_words > max_overlap:\n",
        "                    max_overlap = included_words / total_words\n",
        "            category_b2 = 1 if max_overlap >= 0.5 else 0\n",
        "\n",
        "            # Baseline 3: WordNet Semantic Similarity\n",
        "            semantic_similarity = sentence_similarity(question, context)\n",
        "            category_b3 = 1 if semantic_similarity >= 0.5 else 0\n",
        "\n",
        "            # Write all baseline results and the is_impossible status in a single row for each question\n",
        "            writer.writerow([question_id, category_b1, category_b2, category_b3, is_impossible])\n",
        "\n",
        "            # Update correctness counters\n",
        "            if (category_b1 == 1) == (not is_impossible):\n",
        "                correct_counts['Baseline 1'] += 1\n",
        "            if (category_b2 == 1) == (not is_impossible):\n",
        "                correct_counts['Baseline 2'] += 1\n",
        "            if (category_b3 == 1) == (not is_impossible):\n",
        "                correct_counts['Baseline 3'] += 1\n",
        "\n",
        "            total_questions += 1\n",
        "\n",
        "        # Calculate and write correctness percentages\n",
        "        correctness = {key: (value / total_questions) * 100 for key, value in correct_counts.items()}\n",
        "        writer.writerow(['Correctness %', correctness['Baseline 1'], correctness['Baseline 2'], correctness['Baseline 3'], ''])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4nzxcEIMtp_"
      },
      "source": [
        "Main program run use dev database from SQuAD2.0 (https://rajpurkar.github.io/SQuAD-explorer/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLifSPpiMvyP",
        "outputId": "8115b5a3-e4ac-4e81-ff49-93dc9e698525"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating questions: 100%|██████████| 11873/11873 [41:05<00:00,  4.82it/s] \n"
          ]
        }
      ],
      "source": [
        "# Load your datasets\n",
        "test_json = load_data('../MISC/Dataset/dev-v2.0.json')\n",
        "\n",
        "# Evaluate the semantic similarity baseline\n",
        "evaluate_baseline(test_json, 'combined_baseline_results.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
